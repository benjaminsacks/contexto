{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.data import make_dataset\n",
    "# from src.simulations import run_simulations\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "with open(\"..\\\\data\\\\raw\\\\glove.6B.50d.txt\", encoding=\"utf8\") as f:\n",
    "            for line in tqdm(f, desc=\"Reading file\", unit=\"line\", total=400_000):\n",
    "                word, coefs = line.split(maxsplit=1)\n",
    "                word_list += [word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"len(words_array) (raw):\\t\\t\\t\" + \"{:,}\".format(len(word_list)))\n",
    "word_list = make_dataset.filter_alphabetic(word_list)\n",
    "print(\"len(words_array) (filter_alphabetic):\\t\" + \"{:,}\".format(len(word_list)))\n",
    "word_list = make_dataset.filter_20k(word_list, \"..\\\\data\\\\external\\\\20k.txt\")\n",
    "print(\"len(words_array) (filter_20k):\\t\\t\" + \"{:,}\".format(len(word_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word, tag_num=0):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][tag_num].lower()\n",
    "    if tag in \"nvars\": return tag\n",
    "    else: return \"n\"\n",
    "\n",
    "def lemmatize_words(words):\n",
    "    lemmatized_set = set()\n",
    "    wnl = WordNetLemmatizer()\n",
    "\n",
    "    for word in words:\n",
    "        word_lemma_list = [wnl.lemmatize(word, pos=p) for p in \"nvars\"]\n",
    "        shortest_lemma = min(word_lemma_list, key=len)\n",
    "        if len(wordnet.synsets(shortest_lemma)) > 0:\n",
    "            lemmatized_set.add(shortest_lemma)\n",
    "\n",
    "    return list(lemmatized_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_list = lemmatize_words(word_list)\n",
    "print(len(lemma_list))\n",
    "np.random.choice(lemma_list, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice(lemma_list, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_word = \"incorporates\"\n",
    "print(nltk.pos_tag([test_word])[0])\n",
    "\n",
    "print()\n",
    "print(\"------ Double Lemma: Method 1 ------\")\n",
    "test_pos_1 = get_wordnet_pos(test_word)\n",
    "lemma_1 = wnl.lemmatize(test_word, pos=test_pos_1)\n",
    "test_pos_2 = get_wordnet_pos(lemma_1)\n",
    "lemma_2 = wnl.lemmatize(lemma_1, pos=test_pos_2)\n",
    "\n",
    "print(\"\\ntest_word: \" + test_word + \n",
    "      \"\\tpos: \" + test_pos_1)\n",
    "print(\"1st lemma: \" + lemma_1 + \n",
    "      \"\\tpos: \" + test_pos_2)\n",
    "print(\"2nd lemma: \" + lemma_2)\n",
    "\n",
    "print()\n",
    "print(\"------ Double Lemma: Method 2 ------\")\n",
    "test_pos_1 = get_wordnet_pos(test_word)\n",
    "test_pos_2 = get_wordnet_pos(test_word, tag_num=1)\n",
    "lemma_1 = wnl.lemmatize(test_word, pos=test_pos_1)\n",
    "lemma_2 = wnl.lemmatize(lemma_1, pos=test_pos_2)\n",
    "\n",
    "print(\"\\ntest_word: \" + test_word + \n",
    "      \"\\tpos: \" + test_pos_1)\n",
    "print(\"1st lemma: \" + lemma_1 + \n",
    "      \"\\tpos: \" + test_pos_2)\n",
    "print(\"2nd lemma: \" + lemma_2)\n",
    "\n",
    "print()\n",
    "print(\"------ Double Lemma: Method 3 ------\")\n",
    "test_lemma_list = [wnl.lemmatize(test_word, pos=p) for p in \"nvars\"]\n",
    "\n",
    "print(\"\\ntest_word: \" + test_word)\n",
    "for i, l in enumerate(test_lemma_list):\n",
    "    print(\"pos: \" + \"nvars\"[i] + \"\\tlemma: \" + l)\n",
    "\n",
    "print(\"selected lemma: \" + min(test_lemma_list, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = make_dataset.parse_glove_data(\"..\\\\data\\\\raw\\\\glove.6B.50d.txt\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_array = make_dataset.get_words_array(embeddings_index)\n",
    "print(\"len(words_array) (raw):\\t\\t\\t\" + \"{:,}\".format(len(words_array)))\n",
    "words_array = make_dataset.filter_alphabetic(words_array)\n",
    "print(\"len(words_array) (filter_alphabetic):\\t\" + \"{:,}\".format(len(words_array)))\n",
    "words_array = make_dataset.filter_20k(words_array, \"..\\\\data\\\\external\\\\20k.txt\")\n",
    "print(\"len(words_array) (filter_20k):\\t\\t\" + \"{:,}\".format(len(words_array)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_df = make_dataset.embeddings_to_dataframe(embeddings_index, words_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

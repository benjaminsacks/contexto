{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.data import make_dataset\n",
    "# from src.simulations import run_simulations\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading file: 100%|██████████| 400000/400000 [00:00<00:00, 469047.93line/s]\n"
     ]
    }
   ],
   "source": [
    "word_list = []\n",
    "with open(\"..\\\\data\\\\raw\\\\glove.6B.50d.txt\", encoding=\"utf8\") as f:\n",
    "            for line in tqdm(f, desc=\"Reading file\", unit=\"line\", total=400_000):\n",
    "                word, coefs = line.split(maxsplit=1)\n",
    "                word_list += [word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(words_array) (raw):\t\t\t400,000\n",
      "len(words_array) (filter_alphabetic):\t317,756\n",
      "len(words_array) (filter_20k):\t\t19,737\n"
     ]
    }
   ],
   "source": [
    "print(\"len(words_array) (raw):\\t\\t\\t\" + \"{:,}\".format(len(word_list)))\n",
    "word_list = make_dataset.filter_alphabetic(word_list)\n",
    "print(\"len(words_array) (filter_alphabetic):\\t\" + \"{:,}\".format(len(word_list)))\n",
    "word_list = make_dataset.filter_20k(word_list, \"..\\\\data\\\\external\\\\20k.txt\")\n",
    "print(\"len(words_array) (filter_20k):\\t\\t\" + \"{:,}\".format(len(word_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].lower()\n",
    "    if tag in \"nvars\": return tag\n",
    "    else: return \"n\"\n",
    "\n",
    "def lemmatize_words(words):\n",
    "    lemmatized_set = set()\n",
    "    wnl = WordNetLemmatizer()\n",
    "    [lemmatized_set.add(wnl.lemmatize(word, pos=get_wordnet_pos(word))) for word in words]\n",
    "    return list(lemmatized_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['statement', 'bet', 'zh', 'spectacular', 'dental', 'performs',\n",
       "       'privacy', 'ietf', 'showcase', 'contextual', 'np', 'duplicate',\n",
       "       'nora', 'gently', 'coke', 'devel', 'wager', 'rocker', 'japan',\n",
       "       'budgetary'], dtype='<U17')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_list = lemmatize_words(word_list)\n",
    "print(len(lemma_list))\n",
    "np.random.choice(lemma_list, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n\n",
      "n\n"
     ]
    }
   ],
   "source": [
    "test_word = \"performs\"\n",
    "print(nltk.pos_tag([test_word])[0][1][0].lower())\n",
    "print(get_wordnet_pos(test_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'performs'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "wnl.lemmatize(test_word, pos=\"n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading file:   0%|          | 0/400000 [00:00<?, ?line/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading file:  20%|█▉        | 79837/400000 [00:03<00:15, 20728.04line/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "embeddings_index = make_dataset.parse_glove_data(\"..\\\\data\\\\raw\\\\glove.6B.50d.txt\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(words_array) (raw):\t\t\t400,000\n",
      "len(words_array) (filter_alphabetic):\t317,756\n",
      "len(words_array) (filter_20k):\t\t19,737\n"
     ]
    }
   ],
   "source": [
    "words_array = make_dataset.get_words_array(embeddings_index)\n",
    "print(\"len(words_array) (raw):\\t\\t\\t\" + \"{:,}\".format(len(words_array)))\n",
    "words_array = make_dataset.filter_alphabetic(words_array)\n",
    "print(\"len(words_array) (filter_alphabetic):\\t\" + \"{:,}\".format(len(words_array)))\n",
    "words_array = make_dataset.filter_20k(words_array, \"..\\\\data\\\\external\\\\20k.txt\")\n",
    "print(\"len(words_array) (filter_20k):\\t\\t\" + \"{:,}\".format(len(words_array)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_df = make_dataset.embeddings_to_dataframe(embeddings_index, words_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
